{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based methods \n",
    "In this lab, we will go over how to implement tree-based classification and regression methods. \n",
    "\n",
    "## Goals of the lab:\n",
    "* Learn to use the `tree` library to build \n",
    "classification trees & regression trees\n",
    "* Learn how to use the `randomForest` package to apply bagging and random forests to a sample dataset\n",
    "* Learn to use the `gbm` package to boost trees\n",
    "\n",
    "This lab draws from the practice sets at the end of Chapter 8 in James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). \"An introduction to statistical learning: with applications in r.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pros and cons of trees {the statistical ones}\n",
    "  * Trees are interpretable. They are amenable to visualization and their visualization can be easily interpreted (if they are small)\n",
    "  * Trees can handle qualitative predictors without having to create dummy variables\n",
    "  <br>\n",
    "  <br>\n",
    "\n",
    "  * However, trees do not generally have the same predictive accuracy as other regression and classification approaches\n",
    "  * Trees can be sensitive to small changes in the data (i.e., flexible), so that even small differences in the data used to construct trees can result in drastically different decision trees\n",
    " \n",
    "## Some solutions \n",
    "\n",
    "To minimize these shortcomings, you can aggregate many decision trees using *bagging*, *random forests*, and *boosting*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fitting classification trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/lr/xx8bvmpj6174nnvl_9wvrkhh0000gn/T//RtmpzRFnl5/downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"tree\")\n",
    "library(ISLR)\n",
    "\n",
    "# install.packages('tree') #uncomment to install \n",
    "library(tree) #this is the library needed for tree regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we are going to construct a classification tree using the `Carseats` dataset. Here, `Sales` is a continuous variable, so we will binarize it for compatiblity with classification trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?Carseats\n",
    "# head(Carseats) #if you need a reminder of the structure of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Sales</th><th scope=col>CompPrice</th><th scope=col>Income</th><th scope=col>Advertising</th><th scope=col>Population</th><th scope=col>Price</th><th scope=col>ShelveLoc</th><th scope=col>Age</th><th scope=col>Education</th><th scope=col>Urban</th><th scope=col>US</th><th scope=col>High</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 9.50 </td><td>138   </td><td> 73   </td><td>11    </td><td>276   </td><td>120   </td><td>Bad   </td><td>42    </td><td>17    </td><td>Yes   </td><td>Yes   </td><td>Yes   </td></tr>\n",
       "\t<tr><td>11.22 </td><td>111   </td><td> 48   </td><td>16    </td><td>260   </td><td> 83   </td><td>Good  </td><td>65    </td><td>10    </td><td>Yes   </td><td>Yes   </td><td>Yes   </td></tr>\n",
       "\t<tr><td>10.06 </td><td>113   </td><td> 35   </td><td>10    </td><td>269   </td><td> 80   </td><td>Medium</td><td>59    </td><td>12    </td><td>Yes   </td><td>Yes   </td><td>Yes   </td></tr>\n",
       "\t<tr><td> 7.40 </td><td>117   </td><td>100   </td><td> 4    </td><td>466   </td><td> 97   </td><td>Medium</td><td>55    </td><td>14    </td><td>Yes   </td><td>Yes   </td><td>No    </td></tr>\n",
       "\t<tr><td> 4.15 </td><td>141   </td><td> 64   </td><td> 3    </td><td>340   </td><td>128   </td><td>Bad   </td><td>38    </td><td>13    </td><td>Yes   </td><td>No    </td><td>No    </td></tr>\n",
       "\t<tr><td>10.81 </td><td>124   </td><td>113   </td><td>13    </td><td>501   </td><td> 72   </td><td>Bad   </td><td>78    </td><td>16    </td><td>No    </td><td>Yes   </td><td>Yes   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       " Sales & CompPrice & Income & Advertising & Population & Price & ShelveLoc & Age & Education & Urban & US & High\\\\\n",
       "\\hline\n",
       "\t  9.50  & 138    &  73    & 11     & 276    & 120    & Bad    & 42     & 17     & Yes    & Yes    & Yes   \\\\\n",
       "\t 11.22  & 111    &  48    & 16     & 260    &  83    & Good   & 65     & 10     & Yes    & Yes    & Yes   \\\\\n",
       "\t 10.06  & 113    &  35    & 10     & 269    &  80    & Medium & 59     & 12     & Yes    & Yes    & Yes   \\\\\n",
       "\t  7.40  & 117    & 100    &  4     & 466    &  97    & Medium & 55     & 14     & Yes    & Yes    & No    \\\\\n",
       "\t  4.15  & 141    &  64    &  3     & 340    & 128    & Bad    & 38     & 13     & Yes    & No     & No    \\\\\n",
       "\t 10.81  & 124    & 113    & 13     & 501    &  72    & Bad    & 78     & 16     & No     & Yes    & Yes   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Sales | CompPrice | Income | Advertising | Population | Price | ShelveLoc | Age | Education | Urban | US | High |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "|  9.50  | 138    |  73    | 11     | 276    | 120    | Bad    | 42     | 17     | Yes    | Yes    | Yes    |\n",
       "| 11.22  | 111    |  48    | 16     | 260    |  83    | Good   | 65     | 10     | Yes    | Yes    | Yes    |\n",
       "| 10.06  | 113    |  35    | 10     | 269    |  80    | Medium | 59     | 12     | Yes    | Yes    | Yes    |\n",
       "|  7.40  | 117    | 100    |  4     | 466    |  97    | Medium | 55     | 14     | Yes    | Yes    | No     |\n",
       "|  4.15  | 141    |  64    |  3     | 340    | 128    | Bad    | 38     | 13     | Yes    | No     | No     |\n",
       "| 10.81  | 124    | 113    | 13     | 501    |  72    | Bad    | 78     | 16     | No     | Yes    | Yes    |\n",
       "\n"
      ],
      "text/plain": [
       "  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n",
       "1  9.50 138        73    11          276        120   Bad       42  17       \n",
       "2 11.22 111        48    16          260         83   Good      65  10       \n",
       "3 10.06 113        35    10          269         80   Medium    59  12       \n",
       "4  7.40 117       100     4          466         97   Medium    55  14       \n",
       "5  4.15 141        64     3          340        128   Bad       38  13       \n",
       "6 10.81 124       113    13          501         72   Bad       78  16       \n",
       "  Urban US  High\n",
       "1 Yes   Yes Yes \n",
       "2 Yes   Yes Yes \n",
       "3 Yes   Yes Yes \n",
       "4 Yes   Yes No  \n",
       "5 Yes   No  No  \n",
       "6 No    Yes Yes "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#if sales are <= $8000, then sales are not high \n",
    "# otherwise, they are. \n",
    "High <- as.factor(ifelse(Carseats$Sales<=8, \"No\", \"Yes\") )\n",
    "\n",
    "Carseats['High'] <- High #add the binarized variable to the dataframe\n",
    "head(Carseats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the `tree` function to fit a classification tree. This will predict `High` using all variables except `Sales` (which we transformed above into `High`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.carseats <- tree(High~.-Sales, Carseats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary of the tree will list the variables used as nodes in the tree, the number of terminal nodes, and the training error rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(tree.carseats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the training error rate was 9%. The residual mean deviance is also reported. A small deviance indicates a tree that fits the training data well. The residual mean deviance is the deviance divided by the number of observations ($n$) subtracted from the number of terminal nodes in the tree ($|T_0|$).  \n",
    "\n",
    "In this case, the deviance is $170.7$. There are 400 observations for Sales, and 27 terminal nodes, so $(n-|T_0|)$ is $373$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the tree we've constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(tree.carseats) #this will create the basic tree structure\n",
    "text(tree.carseats, pretty=0) #this will annotate the tree with predictors and outcomes\n",
    "#pretty=0 means that the tree will display the full names of the qualitative predictors instead of \n",
    "#single-letter abbreviations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important indicator of sales seems to be the shelving location, because the first branch splits subsequent branches according to whether shelving location is bad or medium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also just type the tree object name to see output for each branch of the tree. Below, the split criterion is displayed ($Price<92.5$), the number of observations for each branch is shown, the deviance, the overall prediction for that branch, and the number of observations in that branch that take on the values $Yes$ or $No$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.carseats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's estimate the test error of this classification tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed (2)\n",
    "train=sample(1:nrow(Carseats), 200) #50/50 split\n",
    "Carseats.test=Carseats[-train ,] #indices for test set\n",
    "High.test=High[-train ] #get test data\n",
    "tree.carseats=tree(High~.-Sales, Carseats, subset=train ) #train the tree\n",
    "tree.pred=predict(tree.carseats, Carseats.test, type=\"class\") #make a prediction using the test set\n",
    "#class instructs R to return the class prediction\n",
    "table(tree.pred, High.test) #compare predictions with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracy = (86 + 57) / 200 #look at total accuracy\n",
    "total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prune the tree to determine the optimal level of complexity. The function \n",
    "`cv.tree` will perform cross-validation to determine the complexity that maximizes the bias-variance tradeoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(3)\n",
    "cv.carseats = cv.tree(tree.carseats, FUN=prune.misclass) \n",
    "#FUN=prune.misclass indicates that we want classification error rate to guide the \n",
    "#cross-validated pruning process \n",
    "names(cv.carseats) #this function reports the terminal number of nodes \n",
    "#for each tree considered (size), the corresponding error rate (dev),\n",
    "#and the value of the cost complexity parameter (k, corresponding to alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.carseats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the tree with 9 terminal nodes results in the lowest cross-validated error rate, with 50 cross-validation errors. Below is the CV error rate as function of tree size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cv.carseats$size ,cv.carseats$dev ,type=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the `prune.misclass()` function to prune the tree to have the number of nodes to match the best CV error above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.carseats=prune.misclass(tree.carseats,best=9) #getting the pruned tree\n",
    "plot(prune.carseats) #plotting the newly pruned tree\n",
    "text(prune.carseats, pretty=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well this pruned tree classifies test set observations using the `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.pred=predict(prune.carseats, Carseats.test, type=\"class\")\n",
    "table(tree.pred, High.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pruned_accuracy = (94+60)/200\n",
    "total_pruned_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictive classification accuracy has improved and the tree is now more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fitting regression trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit a regression tree to the `Boston` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "head(Boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "train = sample(1: nrow(Boston), nrow(Boston)/2) #create the training set\n",
    "tree.boston=tree(medv~.,Boston, subset=train) #fit the regression tree\n",
    "summary(tree.boston) #summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output of summary() indicates that only three of the variables\n",
    "have been used in constructing the tree. \n",
    "\n",
    "For a regression tree, the deviance is just the sum of the squared errors for the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(tree.boston)\n",
    "text(tree.boston, pretty=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable lstat measures the percentage of individuals with lower\n",
    "socioeconomic status. The tree indicates that lower values of lstat correspond\n",
    "to more expensive houses. The tree predicts a median house price\n",
    "of $46,400 for larger homes in suburbs in which residents have high socioeconomic\n",
    "status (rm>=7.437 and lstat<9.715).\n",
    "\n",
    "Now let's prune the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, let's look at the CV error as a function tree size\n",
    "cv.boston=cv.tree(tree.boston) \n",
    "plot(cv.boston$size, cv.boston$dev,type='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these CV results, we will use the unpruned tree to make predictions on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat=predict(tree.boston,newdata=Boston[-train ,])\n",
    "boston.test=Boston[-train, \"medv\"]\n",
    "plot(yhat ,boston.test)\n",
    "abline(0,1)\n",
    "mean((yhat-boston.test)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE associated with the regression tree is\n",
    "25.05. The square root of the MSE is therefore around 5.005, indicating\n",
    "that this model leads to test predictions that are within around $5, 005 of\n",
    "the true median home value for the suburb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: In this case, the most complex tree was selected by cross-validation. However, you can manually prune the tree using the following and subsequently use a pruned tree for test set predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.boston=prune.tree(tree.boston,best=5) #manually selecting a 5-node tree\n",
    "plot(prune.boston)\n",
    "text(prune.boston,pretty=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply bagging and random forests to the `Boston` dataset using the `randomForest` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is just a special case of a random forest with $m=p$, so the randomForest() function can be used for both purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages('randomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(randomForest)\n",
    "set.seed(1)\n",
    "\n",
    "#the arg. mtry=13 indicates that all 13 predictors should be considered\n",
    "#for each split of the tree (i.e., bagging should be done, m=p)\n",
    "bag.boston=randomForest(medv~.,data=Boston, subset=train, mtry=13,\n",
    "                       importance=TRUE)\n",
    "bag.boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set performance of this bagged regression\n",
    "yhat.bag = predict (bag.boston, newdata=Boston[-train ,])\n",
    "plot(yhat.bag, boston.test)\n",
    "abline(0,1)\n",
    "mean((yhat.bag - boston.test)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE associated with the bagged regression tree is ~13, almost\n",
    "half that obtained using an optimally-pruned single tree. We could change\n",
    "the number of trees grown by randomForest() using the ntree argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.boston=randomForest(medv~.,data=Boston, subset=train, mtry=13,\n",
    "                       importance=TRUE, ntree=25) #grow 25 trees\n",
    "yhat.bag = predict (bag.boston,newdata=Boston[-train ,])\n",
    "mean((yhat.bag-boston.test)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Growing a random forest instead is implemented similarly. We just use a smaller value for the `mtry` argument. Note that, by default, randomForest() will use $p$/3 variables for building a random forest of regression trees, and $\\sqrt p$ variables for building a random forest of classification trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "rf.boston=randomForest(medv~.,data=Boston, subset=train ,\n",
    "mtry=6, importance=TRUE)\n",
    "yhat.rf = predict(rf.boston, newdata=Boston[-train ,])\n",
    "mean((yhat.rf-boston.test)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE is ~11. Random forests yielded an\n",
    "improvement over bagging in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance() function shows the importance of each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance(rf.boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows two metrics of importance, %IncMSE and IncNodePurity (see ISLR, p. 330 for explanation).\n",
    "\n",
    "You can visualize the relative importance of each variable using the below function. Here, we see that lstat (wealth of community) and house size (rm) are the two most important predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varImpPlot(rf.boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use the `gbm` package and the `gbm()` function to fit boosted regression trees using hte Boston dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages('gbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(gbm)\n",
    "set.seed (1)\n",
    "boost.boston =gbm(medv~.,data=Boston[train ,], distribution=\n",
    "                  \"gaussian\", n.trees=5000, interaction.depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use gbm() with\n",
    "distribution=\"gaussian\" because this is a regression problem. If it were binary\n",
    "classification, we would use distribution=\"bernoulli\". The\n",
    "argument n.trees=5000 means that we want 5000 trees. The option\n",
    "interaction.depth=4 limits the depth of each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary will produce a table of relative influence statistics for each variable, and a relative influence plot.\n",
    "\n",
    "You can see that lstat and rm are the most important variables.\n",
    "\n",
    "See p. 331 for an extension of this called partial dependence plots. These plots show the marginal effect of selected variables on the outcome after accounting for the effect of the other variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(boost.boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the boosted model to predict mdev on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.boost=predict(boost.boston,newdata=Boston[-train ,],\n",
    "n.trees=5000)\n",
    "mean((yhat.boost - boston.test)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test MSE obtained is 10.8, which is similar to the test MSE for random forests\n",
    "and better than the MSE we calculated for bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: You can also construct boosted trees using a different value of the shrinkage parameter, $\\lambda$, by specifying the shrinkage argument within the `gbm()` function. For example:\n",
    "\n",
    "`boost.boston = gbm(medvâˆ¼.,data=Boston[train ,], distribution=\n",
    "\"gaussian \",n.trees=5000 , interaction.depth=4, shrinkage=0.2,\n",
    "verbose=F)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification trees \n",
    "\n",
    "1) This problem involves the OJ data set which is part of the ISLR\n",
    "package.\n",
    "\n",
    "(a) Create a training set containing a random sample of 800 observations,\n",
    "and a test set containing the remaining observations.\n",
    "\n",
    "(b) Fit a tree to the training data, with Purchase as the response\n",
    "and the other variables as predictors. Use the summary() function\n",
    "to produce summary statistics about the tree, and describe the\n",
    "results obtained. What is the training error rate? How many\n",
    "terminal nodes does the tree have?\n",
    "\n",
    "(c) Type in the name of the tree object in order to get a detailed\n",
    "text output. Pick one of the terminal nodes, and interpret the\n",
    "information displayed.\n",
    "\n",
    "(d) Create a plot of the tree, and interpret the results.\n",
    "\n",
    "(e) Predict the response on the test data, and produce a confusion\n",
    "matrix comparing the test labels to the predicted test labels.\n",
    "What is the test error rate?\n",
    "\n",
    "(f) Apply the cv.tree() function to the training set in order to\n",
    "determine the optimal tree size.\n",
    "\n",
    "(g) Produce a plot with tree size on the x-axis and cross-validated\n",
    "classification error rate on the y-axis.\n",
    "\n",
    "(h) Which tree size corresponds to the lowest cross-validated classification\n",
    "error rate?\n",
    "\n",
    "(i) Produce a pruned tree corresponding to the optimal tree size\n",
    "obtained using cross-validation. If cross-validation does not lead\n",
    "to selection of a pruned tree, then create a pruned tree with five\n",
    "terminal nodes.\n",
    "\n",
    "(j) Compare the training error rates between the pruned and unpruned\n",
    "trees. Which is higher?\n",
    "\n",
    "(k) Compare the test error rates between the pruned and unpruned\n",
    "trees. Which is higher?\n",
    "\n",
    "## Regression trees \n",
    "2) We applied a classification tree to the Carseats data set after\n",
    "converting Sales into a qualitative response variable. Now we will\n",
    "seek to predict Sales using regression trees,\n",
    "treating the response as a quantitative variable.\n",
    "<br><br>\n",
    "(a) Split the data set into a training set and a test set. <br><br>\n",
    "(b) Fit a regression tree to the training set. Plot the tree, and interpret\n",
    "the results. What test MSE do you obtain?<br><br>\n",
    "(c) Use cross-validation in order to determine the optimal level of\n",
    "tree complexity. Does pruning the tree improve the test MSE?<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
